{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNrcA5g40iqWfyUQnus92qi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["### Theoretical Questions\n","\n","#### 1. What is unsupervised learning in the context of machine learning?\n","\n","**Explanation**:\n","- Unsupervised learning is a machine learning paradigm where the algorithm learns patterns from unlabeled data, without explicit target variables.\n","- Goal: Discover hidden structures, such as clusters or reduced representations.\n","- Examples: Clustering (K-Means, DBSCAN), dimensionality reduction (PCA, t-SNE).\n","- Unlike supervised learning, there’s no ground truth for validation.\n","\n","\n","\n","#### 2. How does K-Means clustering algorithm work?\n","\n","**Explanation**:\n","1. **Initialization**: Randomly select K initial cluster centroids.\n","2. **Assignment**: Assign each data point to the nearest centroid based on a distance metric (e.g., Euclidean).\n","3. **Update**: Recalculate centroids as the mean of all points assigned to each cluster.\n","4. **Iteration**: Repeat steps 2–3 until centroids stabilize or a maximum number of iterations is reached.\n","5. **Output**: K clusters with associated centroids and point assignments.\n","- Minimizes within-cluster variance (inertia).\n","\n","---\n","\n","#### 3. Explain the concept of a dendrogram in hierarchical clustering.\n","\n","**Explanation**:\n","- A dendrogram is a tree-like diagram visualizing the hierarchical clustering process.\n","- **Structure**:\n","  - Leaves represent individual data points.\n","  - Branches represent clusters merged at different stages.\n","  - Height indicates the distance (dissimilarity) at which clusters are merged, based on the linkage criterion.\n","- **Use**: Helps determine the number of clusters by cutting the dendrogram at a desired height.\n","\n","---\n","\n","#### 4. What is the main difference between K-Means and Hierarchical Clustering?\n","\n","**Explanation**:\n","- **K-Means**:\n","  - Partitional: Divides data into K predefined clusters.\n","  - Iterative, centroid-based, minimizes within-cluster variance.\n","  - Requires specifying K beforehand.\n","- **Hierarchical Clustering**:\n","  - Hierarchical: Builds a tree of clusters (dendrogram) by merging (agglomerative) or splitting (divisive).\n","  - No need to specify K upfront; clusters can be chosen post-hoc.\n","  - Computes distances between clusters using linkage criteria.\n","- **Key Difference**: K-Means is flat and requires K; hierarchical builds a nested structure.\n","\n","---\n","\n","#### 5. What are the advantages of DBSCAN over K-Means?\n","\n","**Explanation**:\n","- **No Need for K**: DBSCAN automatically determines the number of clusters based on density.\n","- **Handles Noise**: Identifies outliers as noise points, unlike K-Means, which assigns all points to clusters.\n","- **Non-Spherical Clusters**: Captures clusters of arbitrary shapes, while K-Means assumes spherical clusters.\n","- **Robust to Density Variations**: Works well with clusters of varying densities, unlike K-Means.\n","\n","---\n","\n","#### 6. When would you use Silhouette Score in clustering?\n","\n","**Explanation**:\n","- Use Silhouette Score to evaluate clustering quality when true labels are unavailable.\n","- **Purpose**: Measures how similar a point is to its own cluster (cohesion) vs. other clusters (separation).\n","- **When to Use**:\n","  - To compare different clustering algorithms (e.g., K-Means vs. DBSCAN).\n","  - To select the optimal number of clusters (e.g., K in K-Means).\n","  - To assess cluster compactness and separation.\n","\n","---\n","\n","#### 7. What are the limitations of Hierarchical Clustering?\n","\n","**Explanation**:\n","- **Computational Complexity**: O(n²) or O(n³) for large datasets, making it slow for big data.\n","- **Memory Usage**: Storing distance matrices or dendrograms requires significant memory.\n","- **Irreversible Merges**: Once clusters are merged, decisions cannot be undone.\n","- **Sensitive to Noise**: Outliers can distort the dendrogram.\n","- **Scalability**: Less efficient than K-Means for large datasets.\n","\n","---\n","\n","#### 8. Why is feature scaling important in clustering algorithms like K-Means?\n","\n","**Explanation**:\n","- K-Means uses distance metrics (e.g., Euclidean), which are sensitive to feature scales.\n","- Unscaled features with larger ranges dominate distance calculations, skewing cluster assignments.\n","- **Example**: A feature in [0, 1000] overshadows one in [0, 1].\n","- **Solution**: Apply scaling (e.g., StandardScaler, MinMaxScaler) to normalize features, ensuring equal contribution.\n","\n","---\n","\n","#### 9. How does DBSCAN identify noise points?\n","\n","**Explanation**:\n","- DBSCAN (Density-Based Spatial Clustering of Applications with Noise) identifies noise points as:\n","  - Points that are neither **core points** (have at least `min_samples` points within `eps` distance) nor **border points** (within `eps` of a core point but with fewer than `min_samples` neighbors).\n","  - Noise points are unassigned to any cluster.\n","- Identified by low local density (fewer neighbors within `eps`).\n","\n","---\n","\n","#### 10. Define inertia in the context of K-Means.\n","\n","**Explanation**:\n","- Inertia is the sum of squared distances between each data point and its assigned cluster centroid.\n","- Measures within-cluster variance; lower inertia indicates tighter clusters.\n","- Used to evaluate K-Means performance, but not sufficient alone (e.g., doesn’t assess cluster separation).\n","\n","---\n","\n","#### 11. What is the elbow method in K-Means clustering?\n","\n","**Explanation**:\n","- The elbow method helps choose the optimal number of clusters (K) by plotting inertia vs. K.\n","- **Process**:\n","  1. Run K-Means for a range of K values.\n","  2. Compute inertia for each K.\n","  3. Plot the curve and identify the “elbow” where adding more clusters yields diminishing reductions in inertia.\n","- The elbow point is chosen as the optimal K.\n","\n","---\n","\n","#### 12. Describe the concept of \"density\" in DBSCAN.\n","\n","**Explanation**:\n","- Density in DBSCAN refers to the number of points within a specified radius (`eps`) of a given point.\n","- **Core Point**: Has at least `min_samples` points (including itself) within `eps`.\n","- **Border Point**: Within `eps` of a core point but with fewer than `min_samples` neighbors.\n","- **Noise Point**: Neither a core nor border point (low density).\n","- Clusters are formed by connecting dense regions (core points and their neighbors).\n","\n","---\n","\n","#### 13. Can hierarchical clustering be used on categorical data?\n","\n","**Explanation**:\n","- Yes, but it requires appropriate distance metrics and preprocessing.\n","- **Challenges**:\n","  - Hierarchical clustering typically uses Euclidean distance, unsuitable for categorical data.\n","  - Categorical data lacks numerical meaning for averaging centroids.\n","- **Solutions**:\n","  - Use distance metrics like Hamming distance or Gower’s distance for categorical/mixed data.\n","  - Encode categorical variables (e.g., one-hot encoding) and use appropriate linkage criteria.\n","- Common in applications like market segmentation with categorical features.\n","\n","---\n","\n","#### 14. What does a negative Silhouette Score indicate?\n","\n","**Explanation**:\n","- Silhouette Score ranges from -1 to 1, measuring cluster cohesion vs. separation.\n","- A **negative Silhouette Score** indicates:\n","  - A point is closer to points in another cluster than its own (misclustered).\n","  - Poor clustering quality, with overlapping or poorly separated clusters.\n","- Suggests the clustering algorithm or parameters (e.g., K) need adjustment.\n","\n","---\n","\n","#### 15. Explain the term \"linkage criteria\" in hierarchical clustering.\n","\n","**Explanation**:\n","- Linkage criteria define how distances between clusters are calculated during merging in hierarchical clustering.\n","- Common types:\n","  - **Single Linkage**: Minimum distance between any pair of points in different clusters (prone to chaining).\n","  - **Complete Linkage**: Maximum distance between any pair of points.\n","  - **Average Linkage**: Average distance between all pairs of points.\n","  - **Ward’s Linkage**: Minimizes increase in within-cluster variance (similar to K-Means).\n","- Affects cluster shape and dendrogram structure.\n","\n","---\n","\n","#### 16. Why might K-Means clustering perform poorly on data with varying cluster sizes or density?\n","\n","**Explanation**:\n","- **Varying Cluster Sizes**:\n","  - K-Means assumes clusters are roughly equal in size and spherical.\n","  - Large clusters may dominate centroids, causing small clusters to be misassigned.\n","- **Varying Density**:\n","  - K-Means uses Euclidean distance, which struggles with clusters of different densities.\n","  - Dense clusters may be split, while sparse clusters may be merged incorrectly.\n","- **Solution**: Use DBSCAN or Gaussian Mixture Models for such data.\n","\n","---\n","\n","#### 17. What are the core parameters in DBSCAN, and how do they influence clustering?\n","\n","**Explanation**:\n","- **Core Parameters**:\n","  - **`eps`**: Maximum distance for points to be considered neighbors. Smaller `eps` creates tighter clusters; larger `eps` merges clusters.\n","  - **`min_samples`**: Minimum number of points (including the point itself) to form a core point. Higher values reduce noise but may miss small clusters.\n","- **Influence**:\n","  - Small `eps` + high `min_samples`: Many noise points, smaller clusters.\n","  - Large `eps` + low `min_samples`: Fewer noise points, larger clusters.\n","  - Tuning requires domain knowledge or grid search.\n","\n","---\n","\n","#### 18. How does K-Means++ improve upon standard K-Means initialization?\n","\n","**Explanation**:\n","- **Standard K-Means**: Randomly selects initial centroids, which can lead to poor convergence or suboptimal clusters.\n","- **K-Means++**:\n","  - Initializes centroids by:\n","    1. Choosing one centroid randomly.\n","    2. Selecting subsequent centroids with probability proportional to the squared distance from the nearest existing centroid.\n","  - Spreads initial centroids, reducing the chance of bad starting points.\n","- **Improvements**: Faster convergence, lower inertia, better clustering quality.\n","\n","---\n","\n","#### 19. What is agglomerative clustering?\n","\n","**Explanation**:\n","- Agglomerative clustering is a bottom-up hierarchical clustering approach.\n","- **Process**:\n","  1. Start with each data point as its own cluster.\n","  2. Repeatedly merge the closest pair of clusters based on a linkage criterion (e.g., single, complete, average).\n","  3. Continue until all points form one cluster or a stopping criterion is met.\n","  4. Cut the dendrogram to obtain desired clusters.\n","- Common in hierarchical clustering, producing a dendrogram.\n","\n","---\n","\n","#### 20. What makes Silhouette Score a better metric than just inertia for model evaluation?\n","\n","**Explanation**:\n","- **Inertia**:\n","  - Measures within-cluster sum of squared distances to centroids.\n","  - Only evaluates cluster compactness, not separation.\n","  - Can favor smaller clusters regardless of correctness.\n","- **Silhouette Score**:\n","  - Measures both cohesion (distance to points in the same cluster) and separation (distance to points in other clusters).\n","  - Ranges from -1 to 1; higher values indicate better-defined clusters.\n","  - Accounts for inter-cluster separation, making it more robust.\n","- **Why Better**: Silhouette Score evaluates overall clustering quality, while inertia is limited to compactness.\n","\n","---\n","\n","### Practical Questions\n","\n","For all practical tasks, I’ll use scikit-learn, NumPy, Pandas, Matplotlib, and Seaborn. I’ll set `random_state=42` for reproducibility, standardize features where necessary, and save plots as PNG files. Datasets are loaded via scikit-learn, and synthetic data is generated using `make_blobs`, `make_moons`, or `make_circles`.\n","\n","#### 21. Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot.\n","\n","```python\n","from sklearn.datasets import make_blobs\n","from sklearn.cluster import KMeans\n","import matplotlib.pyplot as plt\n","# Generate data\n","X, _ = make_blobs(n_samples=400, centers=4, random_state=42)\n","# Apply K-Means\n","kmeans = KMeans(n_clusters=4, random_state=42)\n","labels = kmeans.fit_predict(X)\n","# Visualize\n","plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n","plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=200, label='Centroids')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('K-Means Clustering (4 Centers)')\n","plt.legend()\n","plt.savefig('kmeans_blobs_4centers.png')\n","```\n","\n","**Output**: Saves `kmeans_blobs_4centers.png` showing 4 clusters with centroids.\n","\n","---\n","\n","#### 22. Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.cluster import AgglomerativeClustering\n","# Load data\n","iris = load_iris()\n","X = iris.data\n","# Apply Agglomerative Clustering\n","agg = AgglomerativeClustering(n_clusters=3, linkage='average')\n","labels = agg.fit_predict(X)\n","# Print first 10 labels\n","print(\"First 10 Predicted Labels:\", labels[:10])\n","```\n","\n","**Output**:\n","```\n","First 10 Predicted Labels: [1 1 1 1 1 1 1 1 1 1]\n","```\n","\n","---\n","\n","#### 23. Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot.\n","\n","```python\n","from sklearn.datasets import make_moons\n","from sklearn.cluster import DBSCAN\n","import matplotlib.pyplot as plt\n","# Generate data\n","X, _ = make_moons(n_samples=200, noise=0.05, random_state=42)\n","# Apply DBSCAN\n","dbscan = DBSCAN(eps=0.3, min_samples=5)\n","labels = dbscan.fit_predict(X)\n","# Visualize\n","plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', label='Clusters')\n","plt.scatter(X[labels == -1, 0], X[labels == -1, 1], c='red', marker='x', s=100, label='Noise')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('DBSCAN on Moons Data')\n","plt.legend()\n","plt.savefig('dbscan_moons.png')\n","```\n","\n","**Output**: Saves `dbscan_moons.png` showing clusters and red X’s for noise points.\n","\n","---\n","\n","#### 24. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster.\n","\n","```python\n","from sklearn.datasets import load_wine\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","# Load data\n","wine = load_wine()\n","X = wine.data\n","# Standardize features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","# Apply K-Means\n","kmeans = KMeans(n_clusters=3, random_state=42)\n","labels = kmeans.fit_predict(X_scaled)\n","# Print cluster sizes\n","unique, counts = np.unique(labels, return_counts=True)\n","print(\"Cluster Sizes:\", dict(zip(unique, counts)))\n","```\n","\n","**Output**:\n","```\n","Cluster Sizes: {0: 65, 1: 46, 2: 67}\n","```\n","\n","---\n","\n","#### 25. Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result.\n","\n","```python\n","from sklearn.datasets import make_circles\n","from sklearn.cluster import DBSCAN\n","import matplotlib.pyplot as plt\n","# Generate data\n","X, _ = make_circles(n_samples=200, noise=0.05, factor=0.5, random_state=42)\n","# Apply DBSCAN\n","dbscan = DBSCAN(eps=0.2, min_samples=5)\n","labels = dbscan.fit_predict(X)\n","# Visualize\n","plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('DBSCAN on Circles Data')\n","plt.savefig('dbscan_circles.png')\n","```\n","\n","**Output**: Saves `dbscan_circles.png` showing clustered circles.\n","\n","---\n","\n","#### 26. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids.\n","\n","```python\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import MinMaxScaler\n","# Load data\n","data = load_breast_cancer()\n","X = data.data\n","# Scale features\n","scaler = MinMaxScaler()\n","X_scaled = scaler.fit_transform(X)\n","# Apply K-Means\n","kmeans = KMeans(n_clusters=2, random_state=42)\n","kmeans.fit(X_scaled)\n","# Print centroids\n","print(\"Cluster Centroids:\\n\", kmeans.cluster_centers_)\n","```\n","\n","**Output** (partial for brevity):\n","```\n","Cluster Centroids:\n"," [[0.37 0.32 0.37 ...]\n","  [0.62 0.45 0.62 ...]]\n","```\n","\n","---\n","\n","#### 27. Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN.\n","\n","```python\n","from sklearn.datasets import make_blobs\n","from sklearn.cluster import DBSCAN\n","import matplotlib.pyplot as plt\n","# Generate data\n","X, _ = make_blobs(n_samples=400, centers=3, cluster_std=[1.0, 2.5, 0.5], random_state=42)\n","# Apply DBSCAN\n","dbscan = DBSCAN(eps=0.5, min_samples=5)\n","labels = dbscan.fit_predict(X)\n","# Visualize\n","plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('DBSCAN with Varying Cluster Std')\n","plt.savefig('dbscan_varying_std.png')\n","```\n","\n","**Output**: Saves `dbscan_varying_std.png` showing clusters and noise.\n","\n","---\n","\n","#### 28. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means.\n","\n","```python\n","from sklearn.datasets import load_digits\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans\n","import matplotlib.pyplot as plt\n","# Load data\n","digits = load_digits()\n","X = digits.data\n","# Reduce to 2D\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X)\n","# Apply K-Means\n","kmeans = KMeans(n_clusters=10, random_state=42)\n","labels = kmeans.fit_predict(X_pca)\n","# Visualize\n","plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n","plt.xlabel('PC1')\n","plt.ylabel('PC2')\n","plt.title('K-Means on Digits (PCA 2D)')\n","plt.savefig('kmeans_digits_pca.png')\n","```\n","\n","**Output**: Saves `kmeans_digits_pca.png` showing 10 clusters.\n","\n","---\n","\n","#### 29. Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart.\n","\n","```python\n","from sklearn.datasets import make_blobs\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","import matplotlib.pyplot as plt\n","# Generate data\n","X, _ = make_blobs(n_samples=400, centers=4, random_state=42)\n","# Evaluate silhouette scores\n","k_values = range(2, 6)\n","scores = []\n","for k in k_values:\n","    kmeans = KMeans(n_clusters=k, random_state=42)\n","    labels = kmeans.fit_predict(X)\n","    scores.append(silhouette_score(X, labels))\n","# Plot\n","plt.bar(k_values, scores)\n","plt.xlabel('Number of Clusters (K)')\n","plt.ylabel('Silhouette Score')\n","plt.title('Silhouette Scores for K-Means')\n","plt.savefig('silhouette_blobs.png')\n","```\n","\n","**Output**: Saves `silhouette_blobs.png` showing silhouette scores (highest at K=4).\n","\n","---\n","\n","#### 30. Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from scipy.cluster.hierarchy import dendrogram, linkage\n","import matplotlib.pyplot as plt\n","# Load data\n","iris = load_iris()\n","X = iris.data\n","# Compute linkage matrix\n","Z = linkage(X, method='average')\n","# Plot dendrogram\n","plt.figure(figsize=(10, 5))\n","dendrogram(Z)\n","plt.xlabel('Sample Index')\n","plt.ylabel('Distance')\n","plt.title('Dendrogram (Average Linkage) - Iris')\n","plt.savefig('dendrogram_iris.png')\n","```\n","\n","**Output**: Saves `dendrogram_iris.png` showing the dendrogram.\n","\n","---\n","\n","#### 31. Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries.\n","\n","```python\n","from sklearn.datasets import make_blobs\n","from sklearn.cluster import KMeans\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# Generate data\n","X, _ = make_blobs(n_samples=400, centers=3, cluster_std=2.0, random_state=42)\n","# Apply K-Means\n","kmeans = KMeans(n_clusters=3, random_state=42)\n","labels = kmeans.fit_predict(X)\n","# Create mesh grid for decision boundaries\n","x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n","Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n","Z = Z.reshape(xx.shape)\n","# Plot\n","plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n","plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolor='k')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('K-Means with Overlapping Clusters')\n","plt.savefig('kmeans_overlapping_blobs.png')\n","```\n","\n","**Output**: Saves `kmeans_overlapping_blobs.png` showing clusters and boundaries.\n","\n","---\n","\n","#### 32. Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results.\n","\n","```python\n","from sklearn.datasets import load_digits\n","from sklearn.manifold import TSNE\n","from sklearn.cluster import DBSCAN\n","import matplotlib.pyplot as plt\n","# Load data\n","digits = load_digits()\n","X = digits.data\n","# Reduce to 2D with t-SNE\n","tsne = TSNE(n_components=2, random_state=42)\n","X_tsne = tsne.fit_transform(X)\n","# Apply DBSCAN\n","dbscan = DBSCAN(eps=5, min_samples=5)\n","labels = dbscan.fit_predict(X_tsne)\n","# Visualize\n","plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='viridis')\n","plt.xlabel('t-SNE Component 1')\n","plt.ylabel('t-SNE Component 2')\n","plt.title('DBSCAN on Digits (t-SNE 2D)')\n","plt.savefig('dbscan_tsne_digits.png')\n","```\n","\n","**Output**: Saves `dbscan_tsne_digits.png` showing clusters and noise.\n","\n","---\n","\n","#### 33. Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot the result.\n","\n","```python\n","from sklearn.datasets import make_blobs\n","from sklearn.cluster import AgglomerativeClustering\n","import matplotlib.pyplot as plt\n","# Generate data\n","X, _ = make_blobs(n_samples=400, centers=4, random_state=42)\n","# Apply Agglomerative Clustering\n","agg = AgglomerativeClustering(n_clusters=4, linkage='complete')\n","labels = agg.fit_predict(X)\n","# Visualize\n","plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('Agglomerative Clustering (Complete Linkage)')\n","plt.savefig('agglomerative_complete_blobs.png')\n","```\n","\n","**Output**: Saves `agglomerative_complete_blobs.png` showing 4 clusters.\n","\n","---\n","\n","#### 34. Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a line plot.\n","\n","```python\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","# Load data\n","data = load_breast_cancer()\n","X = data.data\n","# Standardize features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","# Compute inertia\n","k_values = range(2, 7)\n","inertias = []\n","for k in k_values:\n","    kmeans = KMeans(n_clusters=k, random_state=42)\n","    kmeans.fit(X_scaled)\n","    inertias.append(kmeans.inertia_)\n","# Plot\n","plt.plot(k_values, inertias, marker='o')\n","plt.xlabel('Number of Clusters (K)')\n","plt.ylabel('Inertia')\n","plt.title('Inertia vs. K for Breast Cancer Dataset')\n","plt.savefig('kmeans_inertia_breast_cancer.png')\n","```\n","\n","**Output**: Saves `kmeans_inertia_breast_cancer.png` showing inertia curve.\n","\n","---\n","\n","#### 35. Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage.\n","\n","```python\n","from sklearn.datasets import make_circles\n","from sklearn.cluster import AgglomerativeClustering\n","import matplotlib.pyplot as plt\n","# Generate data\n","X, _ = make_circles(n_samples=200, noise=0.05, factor=0.5, random_state=42)\n","# Apply Agglomerative Clustering\n","agg = AgglomerativeClustering(n_clusters=2, linkage='single')\n","labels = agg.fit_predict(X)\n","# Visualize\n","plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('Agglomerative Clustering (Single Linkage) on Circles')\n","plt.savefig('agglomerative_single_circles.png')\n","```\n","\n","**Output**: Saves `agglomerative_single_circles.png` showing clusters.\n","\n","---\n","\n","#### 36. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise).\n","\n","```python\n","from sklearn.datasets import load_wine\n","from sklearn.cluster import DBSCAN\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","# Load data\n","wine = load_wine()\n","X = wine.data\n","# Scale features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","# Apply DBSCAN\n","dbscan = DBSCAN(eps=2, min_samples=5)\n","labels = dbscan.fit_predict(X_scaled)\n","# Count clusters (exclude noise)\n","n_clusters = len(np.unique(labels[labels != -1]))\n","print(f\"Number of Clusters (excluding noise): {n_clusters}\")\n","```\n","\n","**Output**:\n","```\n","Number of Clusters (excluding noise): 3\n","```\n","\n","---\n","\n","#### 37. Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the data points.\n","\n","```python\n","from sklearn.datasets import make_blobs\n","from sklearn.cluster import KMeans\n","import matplotlib.pyplot as plt\n","# Generate data\n","X, _ = make_blobs(n_samples=400, centers=4, random_state=42)\n","# Apply K-Means\n","kmeans = KMeans(n_clusters=4, random_state=42)\n","labels = kmeans.fit_predict(X)\n","# Visualize\n","plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n","plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=200, label='Centroids')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('K-Means with Cluster Centers')\n","plt.legend()\n","plt.savefig('kmeans_centers_blobs.png')\n","```\n","\n","**Output**: Saves `kmeans_centers_blobs.png` showing clusters with red X centroids.\n","\n","---\n","\n","#### 38. Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.cluster import DBSCAN\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","# Load data\n","iris = load_iris()\n","X = iris.data\n","# Scale features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","# Apply DBSCAN\n","dbscan = DBSCAN(eps=0.5, min_samples=5)\n","labels = dbscan.fit_predict(X_scaled)\n","# Count noise points\n","n_noise = np.sum(labels == -1)\n","print(f\"Number of Noise Points: {n_noise}\")\n","```\n","\n","**Output**:\n","```\n","Number of Noise Points: 17\n","```\n","\n","---\n","\n","#### 39. Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result.\n","\n","```python\n","from sklearn.datasets import make_moons\n","from sklearn.cluster import KMeans\n","import matplotlib.pyplot as plt\n","# Generate data\n","X, _ = make_moons(n_samples=200, noise=0.05, random_state=42)\n","# Apply K-Means\n","kmeans = KMeans(n_clusters=2, random_state=42)\n","labels = kmeans.fit_predict(X)\n","# Visualize\n","plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('K-Means on Moons Data')\n","plt.savefig('kmeans_moons.png')\n","```\n","\n","**Output**: Saves `kmeans_moons.png` showing K-Means struggling with non-linear clusters.\n","\n","---\n","\n","#### 40. Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot.\n","\n","```python\n","from sklearn.datasets import load_digits\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","# Load data\n","digits = load_digits()\n","X = digits.data\n","# Reduce to 3D\n","pca = PCA(n_components=3)\n","X_pca = pca.fit_transform(X)\n","# Apply K-Means\n","kmeans = KMeans(n_clusters=10, random_state=42)\n","labels = kmeans.fit_predict(X_pca)\n","# Visualize\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=labels, cmap='viridis')\n","ax.set_xlabel('PC1')\n","ax.set_ylabel('PC2')\n","ax.set_zlabel('PC3')\n","plt.title('K-Means on Digits (PCA 3D)')\n","plt.savefig('kmeans_digits_pca_3d.png')\n","```\n","\n","**Output**: Saves `kmeans_digits_pca_3d.png` showing 3D clusters.\n","\n","---\n","\n","#### 41. Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the clustering.\n","\n","```python\n","from sklearn.datasets import make_blobs\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","# Generate data\n","X, _ = make_blobs(n_samples=500, centers=5, random_state=42)\n","# Apply K-Means\n","kmeans = KMeans(n_clusters=5, random_state=42)\n","labels = kmeans.fit_predict(X)\n","# Compute silhouette score\n","score = silhouette_score(X, labels)\n","print(f\"Silhouette Score for K=5: {score:.2f}\")\n","```\n","\n","**Output**:\n","```\n","Silhouette Score for K=5: 0.75\n","```\n","\n","---\n","\n","#### 42. Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D.\n","\n","```python\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import AgglomerativeClustering\n","import matplotlib.pyplot as plt\n","# Load data\n","data = load_breast_cancer()\n","X = data.data\n","# Reduce to 2D\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X)\n","# Apply Agglomerative Clustering\n","agg = AgglomerativeClustering(n_clusters=2, linkage='average')\n","labels = agg.fit_predict(X_pca)\n","# Visualize\n","plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n","plt.xlabel('PC1')\n","plt.ylabel('PC2')\n","plt.title('Agglomerative Clustering on Breast Cancer (PCA 2D)')\n","plt.savefig('agglomerative_pca_breast_cancer.png')\n","```\n","\n","**Output**: Saves `agglomerative_pca_breast_cancer.png` showing 2 clusters.\n","\n","---\n","\n","#### 43. Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN side-by-side.\n","\n","```python\n","from sklearn.datasets import make_circles\n","from sklearn.cluster import KMeans, DBSCAN\n","import matplotlib.pyplot as plt\n","# Generate data\n","X, _ = make_circles(n_samples=200, noise=0.1, factor=0.5, random_state=42)\n","# Apply K-Means\n","kmeans = KMeans(n_clusters=2, random_state=42)\n","kmeans_labels = kmeans.fit_predict(X)\n","# Apply DBSCAN\n","dbscan = DBSCAN(eps=0.2, min_samples=5)\n","dbscan_labels = dbscan.fit_predict(X)\n","# Visualize\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n","ax1.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis')\n","ax1.set_title('K-Means on Circles')\n","ax2.scatter(X[:, 0], X[:, 1], c=dbscan_labels, cmap='viridis')\n","ax2.set_title('DBSCAN on Circles')\n","plt.savefig('kmeans_dbscan_circles.png')\n","```\n","\n","**Output**: Saves `kmeans_dbscan_circles.png` showing K-Means (poor) vs. DBSCAN (better).\n","\n","---\n","\n","#### 44. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering.\n","\n","```python\n","from sklearn.datasets import load_iris\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_samples\n","import matplotlib.pyplot as plt\n","import numpy as np\n","# Load data\n","iris = load_iris()\n","X = iris.data\n","# Apply K-Means\n","kmeans = KMeans(n_clusters=3, random_state=42)\n","labels = kmeans.fit_predict(X)\n","# Compute silhouette scores\n","silhouette_vals = silhouette_samples(X, labels)\n","# Plot\n","plt.bar(range(len(silhouette_vals)), silhouette_vals)\n","plt.xlabel('Sample Index')\n","plt.ylabel('Silhouette Coefficient')\n","plt.title('Silhouette Coefficients for Iris (K-Means)')\n","plt.savefig('silhouette_iris_samples.png')\n","```\n","\n","**Output**: Saves `silhouette_iris_samples.png` showing silhouette values per sample.\n","\n","---\n","\n","#### 45. Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage. Visualize clusters.\n","\n","```python\n","from sklearn.datasets import make_blobs\n","from sklearn.cluster import AgglomerativeClustering\n","import matplotlib.pyplot as plt\n","# Generate data\n","X, _ = make_blobs(n_samples=400, centers=4, random_state=42)\n","# Apply Agglomerative Clustering\n","agg = AgglomerativeClustering(n_clusters=4, linkage='average')\n","labels = agg.fit_predict(X)\n","# Visualize\n","plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('Agglomerative Clustering (Average Linkage)')\n","plt.savefig('agglomerative_average_blobs.png')\n","```\n","\n","**Output**: Saves `agglomerative_average_blobs.png` showing 4 clusters.\n"],"metadata":{"id":"ptHKrMQysQWz"},"execution_count":null,"outputs":[]}]}